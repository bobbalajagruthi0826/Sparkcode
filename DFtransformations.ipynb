{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f53d964-cb92-4725-ac23-0ce204b27a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2984416017994354#setting/sparkui/0324-174914-4s3d4sao/driver-2244547058225954340\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2984416017994354#setting/sparkui/0324-174914-4s3d4sao/driver-2244547058225954340\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a208d45-6b6e-48d8-94c0-e4878dbc3771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n| id|num|\n+---+---+\n|  1|  1|\n|  2|  1|\n|  3|  1|\n|  4|  2|\n|  5|  1|\n|  6|  2|\n|  7|  2|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\t1),\n",
    "(2,\t1),\n",
    "(3,\t1),\n",
    "(4,\t2),\n",
    "(5,\t1),\n",
    "(6,\t2),\n",
    "(7,\t2)]\n",
    "columns =[\"id\",\"num\"]\n",
    "df1= spark.createDataFrame(data,columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50abce75-a02e-4fd4-85c8-1fa4f84aedf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+------+------+\n| id|   name|   dept|salary|  city|\n+---+-------+-------+------+------+\n|  1|  Alice|     HR|  5000|   hyd|\n|  2|    Bob|     IT|   600|  bang|\n|  3|Charlie|Finance|  7000|mumbai|\n|  4|  David|     IT|  5500|  bang|\n|  5|    Eve|     HR|  4800|   hyd|\n+---+-------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "employee_data = [\n",
    "    (1, \"Alice\", \"HR\", 5000,\"hyd\"),\n",
    "    (2, \"Bob\", \"IT\", 600,\"bang\"),\n",
    "    (3, \"Charlie\", \"Finance\", 7000,\"mumbai\"),\n",
    "    (4, \"David\", \"IT\", 5500,\"bang\"),\n",
    "    (5, \"Eve\", \"HR\", 4800,\"hyd\")\n",
    "]\n",
    "employee_schema = StructType([StructField(\"id\",IntegerType(),False),\n",
    "                             StructField(\"name\",StringType(),True),\n",
    "                             StructField(\"dept\",StringType(),True),\n",
    "                             StructField(\"salary\",IntegerType(),True),\n",
    "                             StructField(\"city\",StringType(),True)])\n",
    "emp_df = spark.createDataFrame(employee_data,employee_schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc0dd43-0f3f-4fef-85c2-6af846ebbb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+------+------+\n| id|   name|   dept|salary|  city|\n+---+-------+-------+------+------+\n|  1|  Alice|     HR|  5000|   hyd|\n|  2|    Bob|     IT|   600|  bang|\n|  3|Charlie|Finance|  7000|mumbai|\n|  4|  David|     IT|  5500|  bang|\n|  5|    Eve|     HR|  4800|   hyd|\n+---+-------+-------+------+------+\n\n+-------+\n|   name|\n+-------+\n|  Alice|\n|    Bob|\n|Charlie|\n|  David|\n|    Eve|\n+-------+\n\n+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n+---+\n\n+------+\n|salary|\n+------+\n|  5000|\n|   600|\n|  7000|\n|  5500|\n|  4800|\n+------+\n\n+-------+\n|   dept|\n+-------+\n|     HR|\n|     IT|\n|Finance|\n|     IT|\n|     HR|\n+-------+\n\n+-------+---+------+-------+\n|   name| id|salary|   dept|\n+-------+---+------+-------+\n|  Alice|  1|  5000|     HR|\n|    Bob|  2|   600|     IT|\n|Charlie|  3|  7000|Finance|\n|  David|  4|  5500|     IT|\n|    Eve|  5|  4800|     HR|\n+-------+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#selecting the columns in different ways\n",
    "#select specific columns, and only those coluns appar\n",
    "empdf_selects_all = emp_df.select(\"*\")\n",
    "empdf_name = emp_df.select(\"name\")\n",
    "empdf_id = emp_df.select(col(\"id\"))\n",
    "empdf_slary=emp_df.select(emp_df[\"salary\"])\n",
    "empdf_dept =emp_df.select(emp_df.dept)\n",
    "empdf_fewcol =emp_df.select(\"name\",col(\"id\"),emp_df[\"salary\"],emp_df.dept)\n",
    "empdf_selects_all.show()\n",
    "empdf_name.show()\n",
    "empdf_id.show()\n",
    "empdf_slary.show()\n",
    "empdf_dept.show()\n",
    "empdf_fewcol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94fc241-d40d-4d69-ac3a-cf62914c1b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|(salary + 1000)|\n+---------------+\n|           6000|\n|           1600|\n|           8000|\n|           6500|\n|           5800|\n+---------------+\n\n+---+-------+-------+------+------+---------+\n| id|   name|   dept|salary|  city|newSlaary|\n+---+-------+-------+------+------+---------+\n|  1|  Alice|     HR|  5000|   hyd|     6000|\n|  2|    Bob|     IT|   600|  bang|     1600|\n|  3|Charlie|Finance|  7000|mumbai|     8000|\n|  4|  David|     IT|  5500|  bang|     6500|\n|  5|    Eve|     HR|  4800|   hyd|     5800|\n+---+-------+-------+------+------+---------+\n\n+---+-------+-------+------+------+\n| id|   name|   dept|salary|  city|\n+---+-------+-------+------+------+\n|  1|  Alice|     HR|  6000|   hyd|\n|  2|    Bob|     IT|  1600|  bang|\n|  3|Charlie|Finance|  8000|mumbai|\n|  4|  David|     IT|  6500|  bang|\n|  5|    Eve|     HR|  5800|   hyd|\n+---+-------+-------+------+------+\n\n+---+-------+-------+------+------+------+\n| id|   name|   dept|salary|  city|status|\n+---+-------+-------+------+------+------+\n|  1|  Alice|     HR|  5000|   hyd|     y|\n|  2|    Bob|     IT|   600|  bang|     y|\n|  3|Charlie|Finance|  7000|mumbai|     y|\n|  4|  David|     IT|  5500|  bang|     y|\n|  5|    Eve|     HR|  4800|   hyd|     y|\n+---+-------+-------+------+------+------+\n\n+---+-------+-------+------+------+----------+\n| id|   name|   dept|salary|  city|Upper_name|\n+---+-------+-------+------+------+----------+\n|  1|  Alice|     HR|  5000|   hyd|     ALICE|\n|  2|    Bob|     IT|   600|  bang|       BOB|\n|  3|Charlie|Finance|  7000|mumbai|   CHARLIE|\n|  4|  David|     IT|  5500|  bang|     DAVID|\n|  5|    Eve|     HR|  4800|   hyd|       EVE|\n+---+-------+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Expressions are used to perform computations on DataFrame columns\n",
    "#COLUMN EXPRESSIONS\n",
    "empdf_slary_new1=emp_df.select(col(\"salary\")+1000)\n",
    "empdf_slary_new1.show()\n",
    "#adding new column, updating old column with \"withColumn\"\n",
    "#syntax - df.withColumn(newcolumnname or exisitingcolumnname, expression )\n",
    "empdf_newdf = emp_df.withColumn(\"newSlaary\",col(\"salary\")+1000)\n",
    "empdf_newdf.show()\n",
    "empdf_updateddf = emp_df.withColumn(\"salary\",col(\"salary\")+1000)\n",
    "empdf_updateddf.show()\n",
    "#LITERAL EXPRESSSION\n",
    "empdf_lit = emp_df.select(\"*\",lit(\"y\").alias(\"status\")) #selects everything and adds new column\n",
    "empdf_lit.show()\n",
    "#USER DEFINED EXPRESSSION\n",
    "def upper_func(fname):\n",
    "    return (str.upper(fname))\n",
    "\n",
    "upper_name_udf = udf(upper_func,StringType())\n",
    "\n",
    "empdf_name_upper = emp_df.withColumn(\"Upper_name\",upper_name_udf(col(\"name\")))\n",
    "empdf_name_upper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bcdb5b9-67f8-4eec-ac54-d06410231be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|empid|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n|    5|\n+-----+\n\n+---+-------+-------+------+------+------+\n| id|   name|   dept|salary|  city|emp_id|\n+---+-------+-------+------+------+------+\n|  1|  Alice|     HR|  5000|   hyd|     1|\n|  2|    Bob|     IT|   600|  bang|     2|\n|  3|Charlie|Finance|  7000|mumbai|     3|\n|  4|  David|     IT|  5500|  bang|     4|\n|  5|    Eve|     HR|  4800|   hyd|     5|\n+---+-------+-------+------+------+------+\n\n+---+-------+-------+------+------+\n| id|   name|   dept|salary|  city|\n+---+-------+-------+------+------+\n|  3|Charlie|Finance|  7000|mumbai|\n|  4|  David|     IT|  5500|  bang|\n+---+-------+-------+------+------+\n\n+---+-------+-------+------+------+\n| id|   name|   dept|salary|  city|\n+---+-------+-------+------+------+\n|  3|Charlie|Finance|  7000|mumbai|\n|  4|  David|     IT|  5500|  bang|\n+---+-------+-------+------+------+\n\n+---+-------+-------+------+------+------+\n| id|   name|   dept|salary|  city|status|\n+---+-------+-------+------+------+------+\n|  1|  Alice|     HR|  5000|   hyd|     y|\n|  2|    Bob|     IT|   600|  bang|     y|\n|  3|Charlie|Finance|  7000|mumbai|     y|\n|  4|  David|     IT|  5500|  bang|     y|\n|  5|    Eve|     HR|  4800|   hyd|     y|\n+---+-------+-------+------+------+------+\n\n+---+-------+-------+------+------+------+\n| id|   name|   dept|salary|  city|status|\n+---+-------+-------+------+------+------+\n|  1|  Alice|     HR|  5000|   hyd|     y|\n|  2|    Bob|     IT|   600|  bang|     y|\n|  3|Charlie|Finance|  7000|mumbai|     y|\n|  4|  David|     IT|  5500|  bang|     y|\n|  5|    Eve|     HR|  4800|   hyd|     y|\n+---+-------+-------+------+------+------+\n\n+-------+-------+-------+------+------+\n|emp_idd|   name|   dept|salary|  city|\n+-------+-------+-------+------+------+\n|      1|  Alice|     HR|  5000|   hyd|\n|      2|    Bob|     IT|   600|  bang|\n|      3|Charlie|Finance|  7000|mumbai|\n|      4|  David|     IT|  5500|  bang|\n|      5|    Eve|     HR|  4800|   hyd|\n+-------+-------+-------+------+------+\n\n+-------+-------+------+------+\n|   name|   dept|salary|  city|\n+-------+-------+------+------+\n|  Alice|     HR|  5000|   hyd|\n|    Bob|     IT|   600|  bang|\n|Charlie|Finance|  7000|mumbai|\n|  David|     IT|  5500|  bang|\n|    Eve|     HR|  4800|   hyd|\n+-------+-------+------+------+\n\n+---+-------+------+\n| id|   dept|  city|\n+---+-------+------+\n|  1|     HR|   hyd|\n|  2|     IT|  bang|\n|  3|Finance|mumbai|\n|  4|     IT|  bang|\n|  5|     HR|   hyd|\n+---+-------+------+\n\nroot\n |-- id: string (nullable = false)\n\nroot\n |-- id: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- city: string (nullable = true)\n |-- empidddddd: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "#ALIAS\n",
    "emp_df_alias = emp_df.select(col(\"id\").alias(\"empid\")) #using select\n",
    "emp_df_alias.show()\n",
    "emp_df_alias_with_column = emp_df.withColumn(\"emp_id\",col(\"id\")) #using select\n",
    "emp_df_alias_with_column.show()\n",
    "#filter or Where\n",
    "#syntax - df.filter(column expression)\n",
    "#syntax - df.where(column expression)\n",
    "emp_df_filter = emp_df.filter(col(\"salary\")>5000) #using select\n",
    "emp_df_filter.show()\n",
    "emp_df_where = emp_df.where(col(\"salary\")>5000) #using select\n",
    "emp_df_where.show()\n",
    "#literal\n",
    "empdf_lit = emp_df.select(\"*\",lit(\"y\").alias(\"status\")) #selects everything and adds  \n",
    "                                                        #new column\n",
    "empdf_lit.show()\n",
    "empdf_lit_withcol = emp_df.withColumn(\"status\",lit(\"y\"))\n",
    "empdf_lit_withcol.show()\n",
    "#adding columns\n",
    "#use withColumn to add new column oe update existing columns\n",
    "#renaming columns \n",
    "#syntax  - df.withColumnRenamed(old column name, newcolumn name)\n",
    "emp_df_id_renamed = emp_df.withColumnRenamed(\"id\",\"emp_idd\")\n",
    "emp_df_id_renamed.show()\n",
    "#removing columns\n",
    "#syntax  - df.drop(column names)\n",
    "emp_drop1column = emp_df.drop(\"id\")\n",
    "emp_drop1column.show()\n",
    "emp_dropmulticolumn = emp_df.drop(\"name\",\"salary\")\n",
    "emp_dropmulticolumn.show()\n",
    "#cast columns\n",
    "emp_df_cst= emp_df.select(col('id').cast(\"string\"))\n",
    "emp_df_cst.printSchema()\n",
    "emp_df_cst_withcol= emp_df.withColumn(\"empidddddd\",col('id').cast(\"string\"))\n",
    "emp_df_cst_withcol.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde4b49d-b67c-4a5a-a8d0-9eace2fec60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n+---+------+------+-----+\n\n+---+-------+------+-----+\n| id|   name|salary|marks|\n+---+-------+------+-----+\n|100|  Anila| 50000|   49|\n|101|   Viky| 75000|   20|\n|102|  Nishy| 40000|   45|\n|103|   nick| 60000|   50|\n|104|Pritham| 80000|   18|\n+---+-------+------+-----+\n\n+---+----+------+-----+\n| id|name|salary|marks|\n+---+----+------+-----+\n| 18| Sam| 65000|   33|\n| 18| Sam| 55000|   33|\n| 23| rai| 50000|   46|\n+---+----+------+-----+\n\n+---+----+------+-----+-----+\n| id|name|salary|marks|grade|\n+---+----+------+-----+-----+\n| 10|Anil| 50000|   49|    A|\n|101|Viky| 75000|   20|    B|\n+---+----+------+-----+-----+\n\n+---+------+----+-----+\n| id|salary|name|marks|\n+---+------+----+-----+\n| 10| 50000|Anil|   49|\n|101| 75000|Viky|   20|\n+---+------+----+-----+\n\n+---+-------+------+-----+\n| id|   name|salary|marks|\n+---+-------+------+-----+\n| 10|   Anil| 50000|   49|\n| 11|  Vikas| 75000|   20|\n| 12|  Nisha| 40000|   45|\n| 13|  Nidhi| 60000|   50|\n| 14|  Priya| 80000|   18|\n| 15|  Mohit| 45000|   35|\n| 16| Rajesh| 90000|   43|\n| 17|  Raman| 55000|   39|\n| 18|    Sam| 65000|   33|\n|100|  Anila| 50000|   49|\n|101|   Viky| 75000|   20|\n|102|  Nishy| 40000|   45|\n|103|   nick| 60000|   50|\n|104|Pritham| 80000|   18|\n+---+-------+------+-----+\n\n+---+-------+------+-----+\n| id|   name|salary|marks|\n+---+-------+------+-----+\n| 10|   Anil| 50000|   49|\n| 11|  Vikas| 75000|   20|\n| 12|  Nisha| 40000|   45|\n| 13|  Nidhi| 60000|   50|\n| 14|  Priya| 80000|   18|\n| 15|  Mohit| 45000|   35|\n| 16| Rajesh| 90000|   43|\n| 17|  Raman| 55000|   39|\n| 18|    Sam| 65000|   33|\n|100|  Anila| 50000|   49|\n|101|   Viky| 75000|   20|\n|102|  Nishy| 40000|   45|\n|103|   nick| 60000|   50|\n|104|Pritham| 80000|   18|\n+---+-------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 55000|   33|\n| 23|   rai| 50000|   46|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 55000|   33|\n| 23|   rai| 50000|   46|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 10| 50000|  Anil|   49|\n|101| 75000|  Viky|   20|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 10| 50000|  Anil|   49|\n|101| 75000|  Viky|   20|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 10|  Anil| 50000|   49|\n|101|  Viky| 75000|   20|\n+---+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "#UNION and UNION ALL is same for dataframe\n",
    "#pyspark sql it is different \n",
    "student_data=[(10 ,'Anil',50000, 49),\n",
    "(11 ,'Vikas',75000,  20),\n",
    "(12 ,'Nisha',40000,  45),\n",
    "(13 ,'Nidhi',60000,  50),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  35),\n",
    "(16 ,'Rajesh',90000, 43),\n",
    "(17 ,'Raman',55000, 39),\n",
    "(18 ,'Sam',65000,   33)]\n",
    "student_data2=[(100 ,'Anila',50000, 49),\n",
    "(101 ,'Viky',75000,  20),\n",
    "(102 ,'Nishy',40000,  45),\n",
    "(103 ,'nick',60000,  50),\n",
    "(104 ,'Pritham',80000,  18)]\n",
    "student_data_duplicate=[(18 ,'Sam',65000,   33),\n",
    "(18 ,'Sam',55000,   33),\n",
    "(23,\"rai\",50000,46)]\n",
    "student_data_morecolms=[(10 ,'Anil',50000, 49,\"A\"),\n",
    "(101 ,'Viky',75000,  20,\"B\")]\n",
    "student_data_jumbledcolms=[(10 ,50000,'Anil', 49),\n",
    "(101 ,75000, 'Viky', 20)]\n",
    "student_schema = StructType([StructField(\"id\",IntegerType(),False),\n",
    "                             StructField(\"name\",StringType(),True),\n",
    "                             StructField(\"salary\",IntegerType(),True),\n",
    "                             StructField(\"marks\",IntegerType(),True)])\n",
    "student_schema_more_col = StructType([StructField(\"id\",IntegerType(),False),\n",
    "                             StructField(\"name\",StringType(),True),\n",
    "                             StructField(\"salary\",IntegerType(),True),\n",
    "                             StructField(\"marks\",IntegerType(),True),\n",
    "                             StructField(\"grade\",StringType(),True)])\n",
    "student_schema_jumbled_col = StructType([StructField(\"id\",IntegerType(),False),\n",
    "                            StructField(\"salary\",IntegerType(),True),\n",
    "                             StructField(\"name\",StringType(),True),\n",
    "                             StructField(\"marks\",IntegerType(),True)])\n",
    "\n",
    "mystudentdf1= spark.createDataFrame(student_data,student_schema)\n",
    "mystudentdf1.show()\n",
    "mystudentdf2= spark.createDataFrame(student_data2,student_schema)\n",
    "mystudentdf2.show()\n",
    "mystudentdfduplicate= spark.createDataFrame(student_data_duplicate,student_schema)\n",
    "mystudentdfduplicate.show()\n",
    "mystudentdfmorecol= spark.createDataFrame(student_data_morecolms,student_schema_more_col)\n",
    "mystudentdfmorecol.show()\n",
    "mystudentdfjumbledcolms= spark.createDataFrame(student_data_jumbledcolms,student_schema_jumbled_col)\n",
    "mystudentdfjumbledcolms.show()\n",
    "mystudentdf1.union(mystudentdf2).show()\n",
    "mystudentdf1.unionAll(mystudentdf2).show()\n",
    "mystudentdf1.union(mystudentdfduplicate).show()\n",
    "mystudentdf1.unionAll(mystudentdfduplicate).show()\n",
    "mystudentdf1.union(mystudentdfjumbledcolms).show()\n",
    "mystudentdf1.unionAll(mystudentdfjumbledcolms).show()\n",
    "mystudentdf1.unionByName(mystudentdfjumbledcolms).show()\n",
    "#mystudentdf1.union(mystudentdfmorecol).show() #error\n",
    "#mystudentdf1.unionAll(mystudentdfmorecol).show()#error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f69d99-7e1a-410f-9169-1b140d86762c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----+\n| id|   name|salary|marks|\n+---+-------+------+-----+\n| 10|   Anil| 50000|   49|\n| 11|  Vikas| 75000|   20|\n| 12|  Nisha| 40000|   45|\n| 13|  Nidhi| 60000|   50|\n| 14|  Priya| 80000|   18|\n| 15|  Mohit| 45000|   35|\n| 16| Rajesh| 90000|   43|\n| 17|  Raman| 55000|   39|\n| 18|    Sam| 65000|   33|\n|100|  Anila| 50000|   49|\n|101|   Viky| 75000|   20|\n|102|  Nishy| 40000|   45|\n|103|   nick| 60000|   50|\n|104|Pritham| 80000|   18|\n+---+-------+------+-----+\n\n+---+-------+------+-----+\n| id|   name|salary|marks|\n+---+-------+------+-----+\n| 10|   Anil| 50000|   49|\n| 11|  Vikas| 75000|   20|\n| 12|  Nisha| 40000|   45|\n| 13|  Nidhi| 60000|   50|\n| 14|  Priya| 80000|   18|\n| 15|  Mohit| 45000|   35|\n| 16| Rajesh| 90000|   43|\n| 17|  Raman| 55000|   39|\n| 18|    Sam| 65000|   33|\n|100|  Anila| 50000|   49|\n|101|   Viky| 75000|   20|\n|102|  Nishy| 40000|   45|\n|103|   nick| 60000|   50|\n|104|Pritham| 80000|   18|\n+---+-------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 55000|   33|\n| 23|   rai| 50000|   46|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 65000|   33|\n| 18|   Sam| 55000|   33|\n| 23|   rai| 50000|   46|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 18|   Sam| 65000|   33|\n| 17| Raman| 55000|   39|\n| 10| 50000|  Anil|   49|\n|101| 75000|  Viky|   20|\n+---+------+------+-----+\n\n+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n| 10| 50000|  Anil|   49|\n|101| 75000|  Viky|   20|\n+---+------+------+-----+\n\nUnexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-2433231397931095>\", line 13, in <module>\n    spark.sql(\"SELECT * FROM mystudentdf1_tbl union SELECT * FROM mystudentdfmorecol_tbl\").show()\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/session.py\", line 1387, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 234, in deco\n    raise converted from None\npyspark.errors.exceptions.AnalysisException: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.; line 1 pos 0;\n'Distinct\n+- 'Union false, false\n   :- Project [id#1195, name#1196, salary#1197, marks#1198]\n   :  +- SubqueryAlias mystudentdf1_tbl\n   :     +- View (`mystudentdf1_tbl`, [id#1195,name#1196,salary#1197,marks#1198])\n   :        +- LogicalRDD [id#1195, name#1196, salary#1197, marks#1198], false\n   +- Project [id#1270, name#1271, salary#1272, marks#1273, grade#1274]\n      +- SubqueryAlias mystudentdfmorecol_tbl\n         +- View (`mystudentdfmorecol_tbl`, [id#1270,name#1271,salary#1272,marks#1273,grade#1274])\n            +- LogicalRDD [id#1270, name#1271, salary#1272, marks#1273, grade#1274], false\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.; line 1 pos 0;\n'Distinct\n+- 'Union false, false\n   :- Project [id#1195, name#1196, salary#1197, marks#1198]\n   :  +- SubqueryAlias mystudentdf1_tbl\n   :     +- View (`mystudentdf1_tbl`, [id#1195,name#1196,salary#1197,marks#1198])\n   :        +- LogicalRDD [id#1195, name#1196, salary#1197, marks#1198], false\n   +- Project [id#1270, name#1271, salary#1272, marks#1273, grade#1274]\n      +- SubqueryAlias mystudentdfmorecol_tbl\n         +- View (`mystudentdfmorecol_tbl`, [id#1270,name#1271,salary#1272,marks#1273,grade#1274])\n            +- LogicalRDD [id#1270, name#1271, salary#1272, marks#1273, grade#1274], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "mystudentdf1.createOrReplaceTempView(\"mystudentdf1_tbl\")\n",
    "mystudentdf2.createOrReplaceTempView(\"mystudentdf2_tbl\")\n",
    "mystudentdfduplicate.createOrReplaceTempView(\"mystudentdfduplicate_tbl\")\n",
    "mystudentdfjumbledcolms.createOrReplaceTempView(\"mystudentdfjumbledcolms_tbl\")\n",
    "mystudentdfjumbledcolms.createOrReplaceTempView(\"mystudentdfjumbledcolms_tbl\")\n",
    "mystudentdfmorecol.createOrReplaceTempView(\"mystudentdfmorecol_tbl\")\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union SELECT * FROM mystudentdf2_tbl\").show()\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union all SELECT * FROM mystudentdf2_tbl\").show()\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union SELECT * FROM mystudentdfduplicate_tbl\").show()\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union all SELECT * FROM mystudentdfduplicate_tbl\").show()\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union SELECT * FROM mystudentdfjumbledcolms_tbl\").show()\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union all SELECT * FROM mystudentdfjumbledcolms_tbl\").show()\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union SELECT * FROM mystudentdfmorecol_tbl\").show() #error\n",
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union all SELECT * FROM mystudentdfmorecol_tbl\").show() #error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5e9ecc-869e-4266-b3b4-4e02c2f4e4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----+\n| id|   name|salary|marks|\n+---+-------+------+-----+\n| 10|   Anil| 50000|   49|\n| 11|  Vikas| 75000|   20|\n| 12|  Nisha| 40000|   45|\n| 13|  Nidhi| 60000|   50|\n| 14|  Priya| 80000|   18|\n| 15|  Mohit| 45000|   35|\n| 16| Rajesh| 90000|   43|\n| 17|  Raman| 55000|   39|\n| 18|    Sam| 65000|   33|\n|100|  Anila| 50000|   49|\n|101|   Viky| 75000|   20|\n|102|  Nishy| 40000|   45|\n|103|   nick| 60000|   50|\n|104|Pritham| 80000|   18|\n+---+-------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM mystudentdf1_tbl union SELECT * FROM mystudentdf2_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63db9c1-c2ba-43ba-9f8e-5f825465a4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n| id|  name|salary|marks|\n+---+------+------+-----+\n| 10|  Anil| 50000|   49|\n| 11| Vikas| 75000|   20|\n| 12| Nisha| 40000|   45|\n| 13| Nidhi| 60000|   50|\n| 14| Priya| 80000|   18|\n| 15| Mohit| 45000|   35|\n| 16|Rajesh| 90000|   43|\n| 17| Raman| 55000|   39|\n| 18|   Sam| 65000|   33|\n+---+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#creating table from a data frame\n",
    "#1 creating table locally which is temporary\n",
    "\n",
    "#2 creating table globally which is temporary\n",
    "    mystudentdf1.createGlobalTempView(\"global_mystudentdf1_table\")\n",
    "    # Querying the table using SQL\n",
    "    spark.sql(\"SELECT * FROM global_temp.global_mystudentdf1_table\").show()\n",
    "\n",
    "#3 creating table permanently\n",
    "emp_df.write.mode(\"overwrite\").saveAsTable(\"permanent_employee_table\")\n",
    "   #in specific location\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS company_db\")\n",
    "emp_df.write.mode(\"overwrite\").saveAsTable(\"company_db.employee_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-03-24 11:31:48",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}